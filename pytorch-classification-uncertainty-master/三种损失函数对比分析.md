# 三种EDL损失函数对比分析

基于旋转数字"1"实验的结果图（`rotate_uncertainty_mse.jpg`、`rotate_uncertainty_digamma.jpg`、`rotate_uncertainty_log.jpg`）的分析结论。

## 1. 共同特征

所有三种损失函数都成功实现了不确定性量化，表现出以下共同特点：

- **正确分类区域（0-40°和150-180°）**：
  - 类别"1"的预测概率较高（通常>0.8）
  - 不确定性较低
  
- **错误分类区域（50-150°）**：
  - 类别"1"的预测概率急剧下降至约0.1
  - 不确定性显著增加
  - 模型正确识别出自己无法准确分类的情况

- **不确定性-概率的逆相关关系**：
  - 不确定性曲线与"1"的概率曲线呈现明显的负相关
  - 当概率高时，不确定性低；当概率低时，不确定性高

## 2. 三种损失函数的差异

### 2.1 MSE损失（Expected Mean Square Error, Eq.5）

**特点**：
- 正确分类样本的不确定性：**约0.2**
- 错误分类样本的不确定性：**约1.0**（达到最大值）
- 在错误区域的不确定性达到最大值，表现最为"保守"

**优势**：
- 对于OOD/错误样本的不确定性量化最为敏感
- 不确定性范围较大（0.2到1.0），区分度明显
- 适合需要高敏感度的应用场景

**数学特性**：
- 包含偏差项（期望误差）和方差项（Dirichlet方差）
- 同时考虑预测偏差和模型不确定性

### 2.2 Digamma损失（Expected Cross Entropy, Eq.4）

**特点**：
- 正确分类样本的不确定性：**接近0**（约0.0-0.15）
- 错误分类样本的不确定性：**约0.7**
- 正确分类时不确定性最低，展现出最高的"自信度"

**优势**：
- 对于正确分类的样本，给出非常低的确定性预测
- 不确定性范围适中（0到0.7），平衡了敏感度和稳定性
- 在实际应用中可能更实用，因为正确预测时几乎不产生不确定性

**数学特性**：
- 使用digamma函数（ψ）计算期望交叉熵
- 与softmax交叉熵损失在数学上更加接近

### 2.3 Log损失（Negative Log of Expected Likelihood, Eq.3）

**特点**：
- 不确定性表现介于MSE和Digamma之间
- 正确分类时不确定性相对较低，但可能略高于Digamma
- 错误分类时不确定性较高，但可能略低于MSE的1.0

**优势**：
- 在正确分类和错误分类之间取得平衡
- 可能提供更平滑的不确定性过渡

**数学特性**：
- 基于负对数期望似然
- 与标准对数似然损失函数在形式上相关

## 3. 实际应用建议

### 选择MSE损失的场景：
- 需要最大化OOD检测的敏感性
- 可以接受正确分类时仍有一定的"保守"不确定性（~0.2）
- 错误检测的代价很高，宁愿过度保守

### 选择Digamma损失的场景：
- 需要正确分类时给出高置信度（不确定性接近0）
- 需要平衡的总体表现
- 与标准交叉熵损失在行为上更接近，易于理解和调试

### 选择Log损失的场景：
- 需要在两种极端之间取得平衡
- 需要更平滑的不确定性过渡

## 4. 关键观察

1. **所有方法都有效**：三种损失函数都能有效识别不确定的预测，说明EDL方法具有鲁棒性。

2. **不确定性量化的连续性**：三种方法都展现了平滑的不确定性过渡，从低不确定性（正确分类）到高不确定性（错误分类）。

3. **数学形式的差异影响行为**：
   - MSE：更保守，错误时达到最大不确定性
   - Digamma：更自信，正确时接近零不确定性
   - Log：折中方案

4. **与标准softmax网络的对比**：
   - 标准网络在错误分类时仍给出高概率（过度自信）
   - 所有EDL方法都能正确识别错误并给出高不确定性

## 5. 结论

三种EDL损失函数虽然数学形式不同，但都能有效地量化分类不确定性。主要区别在于：

- **MSE**：最保守，对错误最敏感
- **Digamma**：最自信，对正确预测最确定
- **Log**：平衡方案

选择哪种损失函数应根据具体应用场景的需求来决定。一般来说，**Digamma损失（Eq.4）在实际应用中表现最为平衡和实用**。
